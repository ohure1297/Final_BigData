import streamlit as st
import subprocess
import pandas as pd

def page_mapreduce():
    st.title("üéØ Ph√¢n T√≠ch D·ªØ Li·ªáu Tuy·ªÉn D·ª•ng v·ªõi MapReduce")

    # ƒê·ªãnh nghƒ©a c√°c t√≠nh nƒÉng ph√¢n t√≠ch
    features = {
        "Nh·ªØng c√¥ng vi·ªác c√≥ l∆∞∆°ng h∆°n 10 tri·ªáu": "Luong",
        "Nh·ªØng C√¥ng ty ·ªü H√† N·ªôi ho·∫∑c H·ªì Ch√≠ Minh": "DiaDiem",
        "Nh·ªØng c√¥ng vi·ªác kh√¥ng y√™u c·∫ßu kinh nghi·ªám": "KinhNghiem",
        "Nh·ªØng c√¥ng vi·ªác c·∫ßn ng√¥n ng·ªØ python": "Skill",
        "L∆∞∆°ng trung b√¨nh theo t·ª´ng th√†nh ph·ªë": "TinhTrungBinhLuong",
        "Top 10 c√¥ng vi·ªác c√≥ m·ª©c l∆∞∆°ng cao nh·∫•t": "Top10CV",
        "Nh·ªØng kƒ© nƒÉng tuy·ªÉn d·ª•ng theo s·ªë l∆∞·ª£ng": "KiNangCanThiet",
    }

    # T√™n c·ªôt t∆∞∆°ng ·ª©ng v·ªõi t·ª´ng job
    column_names = {
        "Luong": ["C√¥ng vi·ªác", "L∆∞∆°ng (tri·ªáu VND)"],
        "DiaDiem": ["C√¥ng ty", "Th√†nh ph·ªë"],
        "KinhNghiem": ["C√¥ng vi·ªác", "Kinh Nghiem"],
        "Skill": ["C√¥ng vi·ªác", "Ng√¥n Ng·ªØ"],
        "TinhTrungBinhLuong": ["Th√†nh ph·ªë", "L∆∞∆°ng trung b√¨nh"],
        "Top10CV": ["C√¥ng vi·ªác", "L∆∞∆°ng"],
        "KiNangCanThiet": ["Kƒ© nƒÉng", "S·ªë l∆∞·ª£ng"],
    }

    # Cho ph√©p ng∆∞·ªùi d√πng ch·ªçn t√≠nh nƒÉng ph√¢n t√≠ch
    selected_feature = st.selectbox("Ch·ªçn t√≠nh nƒÉng ph√¢n t√≠ch", list(features.keys()))

    if st.button("Th·ª±c thi"):
        job_folder = features[selected_feature]
        map_path = f"/home/hadoopcong/Final_BigData/my_streamlit_app/mapreduce_jobs/{job_folder}/map.py"
        reduce_path = f"/home/hadoopcong/Final_BigData/my_streamlit_app/mapreduce_jobs/{job_folder}/reduce.py"
        input_path = "/user/hadoopcong/topcv.csv"
        output_path = f"/user/hadoopcong/output/{job_folder}"

        # X√≥a output c≈© n·∫øu t·ªìn t·∫°i
        subprocess.run(["hadoop", "fs", "-rm", "-r", output_path])

        # Th·ª±c thi Hadoop Streaming
        result = subprocess.run(
            ["hadoop", "jar", "/home/hadoopcong/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.4.1.jar",
             "-files", f"{map_path},{reduce_path}",
             "-mapper", f"python3 {map_path}",
             "-reducer", f"python3 {reduce_path}",
             "-input", input_path,
             "-output", output_path],
            stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True
        )

        # In log th·ª±c thi
        st.subheader("üìÑ Log Th·ª±c Thi:")
        st.code(result.stdout + result.stderr)

        if result.returncode != 0:
            st.error(f"‚ùå L·ªói khi ch·∫°y job Hadoop: {result.stderr}")
            return

        # ƒê·ªçc d·ªØ li·ªáu t·ª´ HDFS
        try:
            hdfs_output = subprocess.check_output(
                ["hadoop", "fs", "-cat", f"{output_path}/part-00000"], text=True
            )

            # X·ª≠ l√Ω d·ªØ li·ªáu ƒë·∫ßu ra
            data = []
            for line in hdfs_output.splitlines():
                parts = line.strip().split("\t")
                if len(parts) == 2:
                    key, value = parts
                    try:
                        value = float(value)
                    except ValueError:
                        pass
                    data.append((key, value))

            if data:
                columns = column_names.get(job_folder, ["Key", "Value"])
                df = pd.DataFrame(data, columns=columns)

                st.subheader("üìã D·ªØ li·ªáu chi ti·∫øt")
                st.dataframe(df)
            else:
                st.warning("‚ö†Ô∏è Kh√¥ng c√≥ d·ªØ li·ªáu hi·ªÉn th·ªã.")
        except subprocess.CalledProcessError:
            st.error("‚ùå Kh√¥ng th·ªÉ ƒë·ªçc k·∫øt qu·∫£ t·ª´ HDFS!")
